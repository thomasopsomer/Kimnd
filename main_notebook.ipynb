{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle as pkl\n",
    "from os import path\n",
    "from ast import literal_eval\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import utils # utils is a script for preprocessing the dataset\n",
    "import flat_dataset # to generate positive and negative triplets\n",
    "from gow import tw_idf # an implementation of tw_idf\n",
    "import temporal_features # functions for extracting the temporal features described in the report\n",
    "import textual_features # functions for extracting the textual features\n",
    "from greetings import * # to compute greetings\n",
    "from topic_modeling import * # to compute the LDA\n",
    "\n",
    "from average_precision import mapk # the scoring function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Utils function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def address_book_users(df):\n",
    "    '''\n",
    "    Get the address book (list of contacts) of each user\n",
    "    '''\n",
    "    book = df.groupby(\"sender\").recipients.sum()\n",
    "    book = book.map(lambda x: set(x))\n",
    "    return book\n",
    "\n",
    "\n",
    "def add_recipients(df, all_emails):\n",
    "    \"\"\"\n",
    "    Add recipients to the dataframe df\n",
    "    :param all_emails: all ID contacts of all users\n",
    "    \"\"\"\n",
    "    user = df[\"sender\"].iloc[0] # ID of the user\n",
    "    emails = all_emails[user]\n",
    "    df[\"emails\"] = str(list(emails))\n",
    "    df[\"emails\"] = df[\"emails\"].map(literal_eval)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_test_set(test_user):\n",
    "    \"\"\"\n",
    "    Creates a dataset with all the possible combinations (userID, mid, contactID)\n",
    "    :param test_user: the DataFrame of the test for a specific sender\n",
    "    \"\"\"\n",
    "    test_user = add_recipients(test_user, contacts)\n",
    "    test_user = utils.flatmap(test_user, \"emails\", \"recipient\", np.string_)\n",
    "\n",
    "    # Renaming\n",
    "    test_user = test_user[[\"sender\", \"recipient\", \"mid\", \"body\"]]\n",
    "    return test_user\n",
    "\n",
    "\n",
    "def split_train_dev_set(df, percent=0.2):\n",
    "    \"\"\"\n",
    "    Split dataset in train and dev set\n",
    "    For each sender, we put in the dev set the percentage 'percent' of\n",
    "    the last messages he sent\n",
    "    \"\"\"\n",
    "    train = []\n",
    "    dev = []\n",
    "    for k, g in df.groupby(\"sender\")[\"mid\", \"recipients\"]:\n",
    "        n_msg = g.shape[0]\n",
    "        n_dev = int(n_msg * percent)\n",
    "        g = g.sort_values(\"date\")\n",
    "        g_train = g[:-n_dev]\n",
    "        g_dev = g[-n_dev:]\n",
    "        train.append(g_train)\n",
    "        dev.append(g_dev)\n",
    "    # concat all dataframe\n",
    "    df_train = pd.concat(train, axis=0).sort_index()\n",
    "    df_dev = pd.concat(dev, axis=0).sort_index()\n",
    "    return df_train, df_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA\n",
    "We apply the LDA algorithm and store its results in the folder LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_path = \"data\"\n",
    "compute_lda(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and LDA Loading\n",
    "make sure to have the data in the `data/` folder. and the result of the LDA in the `LDA/` one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TEST = True\n",
    "TYPE_IDF = \"tf_idf\"\n",
    "\n",
    "print \"Loading the files\"\n",
    "dataset_path = \"data/training_set.csv\"\n",
    "dataset_path2 = \"data/test_set.csv\"\n",
    "mail_path = \"data/training_info.csv\"\n",
    "mail_path2 = \"data/test_info.csv\"\n",
    "lda_path = \"LDA/LDA_results.csv\"\n",
    "\n",
    "train_df = utils.load_dataset(dataset_path, mail_path, train=True, flat=True)\n",
    "train_df_not_flat = utils.load_dataset(dataset_path, mail_path, train=True, flat=False)\n",
    "test_df = utils.load_dataset(dataset_path2, mail_path2, train=False)\n",
    "\n",
    "# LDA\n",
    "lda_df = pd.read_csv(lda_path)\n",
    "\n",
    "## TEST\n",
    "if TEST:\n",
    "    train_df_not_flat, test_df = split_train_dev_set(train_df_not_flat, percent=0.06)\n",
    "    train_df = train_df[train_df.mid.isin(train_df_not_flat.mid)]\n",
    "    recips_test = test_df[[\"mid\", \"recipients\"]]\n",
    "    test_df = test_df.drop(\"recipients\", axis=1)\n",
    "\n",
    "print \"Preprocessing messages\"\n",
    "train_df_not_flat = utils.preprocess_bodies(train_df_not_flat, type=\"train\")\n",
    "test_df = utils.preprocess_bodies(test_df, type=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_path = \"time_features.csv\"\n",
    "if TEST:\n",
    "    time_path = \"time_features_DEV.csv\"\n",
    "if path.exists(time_path):\n",
    "    print \"Getting time features\"\n",
    "    time_features = pd.read_csv(time_path)\n",
    "else:\n",
    "    print \"Handling time\"\n",
    "    origine_time = train_df[\"date\"].min()\n",
    "    train_df[\"time\"] = (train_df[\"date\"] - origine_time).apply(lambda x: x.seconds)\n",
    "\n",
    "    print \"Time features extraction\"\n",
    "    time = train_df[\"time\"].max() + 1;\n",
    "    time_features = temporal_features.get_features_out_in(train_df, time)\n",
    "    time_features.to_csv(time_path, sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greetings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Getting the greeting features\"\n",
    "greets, name = search_greetings(train_df_not_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textual features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TW-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if TYPE_IDF == \"tw_idf\":\n",
    "    print \"Extracting global text features\"\n",
    "    idf_path = \"idf.pkl\"\n",
    "    if path.exists(idf_path):\n",
    "        idf = pkl.load(open(idf_path, \"rb\"))\n",
    "        id2word = pkl.load(open(\"id2word.pkl\", \"rb\"))\n",
    "        texts = list(train_df_not_flat[\"tokens\"])\n",
    "        avg_len = sum(len(terms) for terms in texts) / len(texts)\n",
    "    else:\n",
    "        idf, id2word, avg_len = textual_features.get_global_text_features(list(train_df_not_flat[\"tokens\"]))\n",
    "        with open(idf_path, \"w\") as f:\n",
    "            pkl.dump(idf, f)\n",
    "        with open(\"id2word.pkl\", \"w\") as f:\n",
    "            pkl.dump(id2word, f)\n",
    "\n",
    "    print \"Computing and storing tw-idf of all messages\"\n",
    "    pickle_path = \"twidf_dico_train.pkl\"\n",
    "    if TEST:\n",
    "        pickle_path = \"twidf_dico_train_DEV.pkl\"\n",
    "    if path.exists(pickle_path):\n",
    "        idf_dico = pkl.load(open(pickle_path, \"rb\"))\n",
    "    else:\n",
    "        idf_dico = {}\n",
    "        for ind, row in train_df_not_flat.iterrows():\n",
    "            if (ind+1) % 1000 == 0: print \"Processesed \", ind+1\n",
    "            mid = row[\"mid\"]\n",
    "            tokens = row[\"tokens\"]\n",
    "            idf_dico[mid] = tw_idf(tokens, idf, id2word, avg_len, window=5)\n",
    "        with open(pickle_path, \"w\") as f:\n",
    "            pkl.dump(idf_dico, f)\n",
    "\n",
    "    pickle_path = \"twidf_dico_test.pkl\"\n",
    "    if TEST:\n",
    "        pickle_path = \"twidf_dico_test_DEV.pkl\"\n",
    "    if path.exists(pickle_path):\n",
    "        idf_dico_test = pkl.load(open(pickle_path, \"rb\"))\n",
    "    else:\n",
    "        idf_dico_test = {}\n",
    "        for ind, row in test_df.iterrows():\n",
    "            if (ind+1) % 1000 == 0: print \"Processesed \", ind+1\n",
    "            mid = row[\"mid\"]\n",
    "            tokens = row[\"tokens\"]\n",
    "            idf_dico_test[mid] = tw_idf(tokens, idf, id2word, avg_len, window=5)\n",
    "        with open(pickle_path, \"w\") as f:\n",
    "            pkl.dump(idf_dico_test, f)\n",
    "\n",
    "    print \"Getting the averages dictionaries for outgoing and incoming messages\"\n",
    "    # Computes the average tw idf vector (incoming)\n",
    "    dict_tuple_mids_in = train_df.groupby([\"recipient\", \"sender\"])[\"mid\"].apply(list).to_dict()\n",
    "    for tupl in dict_tuple_mids_in.keys():\n",
    "        dict_tuple_mids_in[tupl] = np.average(np.array([idf_dico[m].toarray() for m in dict_tuple_mids_in[tupl]]), axis=0)\n",
    "        dict_tuple_mids_in[tupl] = csr_matrix(dict_tuple_mids_in[tupl])\n",
    "\n",
    "    # Computes the average tw idf vector (outgoing)\n",
    "    dict_tuple_mids_out = train_df.groupby([\"sender\", \"recipient\"])[\"mid\"].apply(list).to_dict()\n",
    "    for tupl in dict_tuple_mids_out.keys():\n",
    "        dict_tuple_mids_out[tupl] = np.average(np.array([idf_dico[m].toarray() for m in dict_tuple_mids_out[tupl]]), axis=0)\n",
    "        dict_tuple_mids_out[tupl] = csr_matrix(dict_tuple_mids_out[tupl])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if TYPE_IDF == \"tf_idf\":\n",
    "\n",
    "    tf = TfidfVectorizer(analyzer=lambda x: x, ngram_range=(1, 1), min_df=5, stop_words='english')\n",
    "\n",
    "    print \"Computing and storing tf-idf of all messages\"\n",
    "    pickle_path = \"tfidf_dico_train.pkl\"\n",
    "    if TEST:\n",
    "        pickle_path = \"tfidf_dico_train_DEV.pkl\"\n",
    "    if path.exists(pickle_path):\n",
    "        idf_dico = pkl.load(open(pickle_path, \"rb\"))\n",
    "    else:\n",
    "        idf_dico = {}\n",
    "        tfidf_matrix = tf.fit_transform(train_df_not_flat.tokens)\n",
    "        ind = 0\n",
    "        for row in train_df_not_flat.iterrows():\n",
    "            if (ind+1) % 1000 == 0: print \"Processed \", ind+1\n",
    "            idf_dico[row[1].mid] = tfidf_matrix[ind]\n",
    "            ind += 1\n",
    "        with open(pickle_path, \"w\") as f:\n",
    "            pkl.dump(idf_dico, f)\n",
    "\n",
    "    pickle_path = \"tfidf_dico_test.pkl\"\n",
    "    if TEST:\n",
    "        pickle_path = \"tfidf_dico_test_DEV.pkl\"\n",
    "    if path.exists(pickle_path):\n",
    "        idf_dico_test = pkl.load(open(pickle_path, \"rb\"))\n",
    "    else:\n",
    "        idf_dico_test = {}\n",
    "        tfidf_matrix_test = tf.transform(test_df.tokens)\n",
    "        ind = 0\n",
    "        for row in test_df.iterrows():\n",
    "            if (ind+1) % 1000 == 0: print \"Processed \", ind+1\n",
    "            idf_dico_test[row[1].mid] = tfidf_matrix_test[ind]\n",
    "            ind += 1\n",
    "        with open(pickle_path, \"w\") as f:\n",
    "            pkl.dump(idf_dico_test, f)\n",
    "    print \"Getting the averages dictionaries for outgoing and incoming messages\"\n",
    "    # Computes the average tw idf vector (incoming)\n",
    "    dict_tuple_mids_in = train_df.groupby([\"recipient\", \"sender\"])[\"mid\"].apply(list).to_dict()\n",
    "    for tupl in dict_tuple_mids_in.keys():\n",
    "        dict_tuple_mids_in[tupl] = np.average(np.array([idf_dico[m].toarray() for m in dict_tuple_mids_in[tupl]]), axis=0)\n",
    "        dict_tuple_mids_in[tupl] = csr_matrix(dict_tuple_mids_in[tupl])\n",
    "\n",
    "    # Computes the average tw idf vector (outgoing)\n",
    "    dict_tuple_mids_out = train_df.groupby([\"sender\", \"recipient\"])[\"mid\"].apply(list).to_dict()\n",
    "    for tupl in dict_tuple_mids_out.keys():\n",
    "        dict_tuple_mids_out[tupl] = np.average(np.array([idf_dico[m].toarray() for m in dict_tuple_mids_out[tupl]]), axis=0)\n",
    "        dict_tuple_mids_out[tupl] = csr_matrix(dict_tuple_mids_out[tupl])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Preparing for the ranking\"\n",
    "# Extract all the emails of the database\n",
    "emails = set(train_df[\"sender\"]).union(set(train_df[\"recipient\"]))\n",
    "\n",
    "# Get all contacts for each user\n",
    "contacts = time_features.groupby(\"user\").contact.apply(set)\n",
    "\n",
    "print \"Generating positive and negative pairs\"\n",
    "# Get the positive and negative pairs for the classifier\n",
    "pairs_train = flat_dataset.make_flat_dataset(train_df_not_flat, contacts, 1.0, num_cores=4)\n",
    "\n",
    "# Adding textual features\n",
    "print \"Textual features for the train pairs\"\n",
    "pairs_train['outgoing_txt'] = textual_features.text_similarity_new(\n",
    "    pairs_train, idf_dico, dict_tuple_mids_out)\n",
    "pairs_train['incoming_txt'] = textual_features.text_similarity_new(\n",
    "    pairs_train, idf_dico, dict_tuple_mids_in)\n",
    "\n",
    "# Greetings\n",
    "greeting_feature = np.empty(pairs_train['incoming_txt'].shape[0])\n",
    "ind = 0\n",
    "for row in pairs_train.itertuples():\n",
    "    greeting_feature[ind] = greeting_value(\n",
    "            row.body, row.recipient, greets, name)\n",
    "    ind += 1\n",
    "pairs_train[\"greet\"] = greeting_feature\n",
    "\n",
    "\n",
    "# Renaming\n",
    "pairs_train = pairs_train.rename(columns={\"sender\":\"user\", \"recipient\": \"contact\"})\n",
    "pairs_train = pairs_train[[\"user\", \"contact\", \"mid\", \"incoming_txt\", \"outgoing_txt\", \"label\", \"greet\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Getting the test set ready\"\n",
    "test_pairs = test_df.groupby(\"sender\").apply(\n",
    "    lambda test_user: get_test_set(test_user))\n",
    "test_pairs = test_pairs.reset_index(drop=True)\n",
    "\n",
    "print \"Adding textual features to the test set\"\n",
    "test_pairs['outgoing_txt'] = textual_features.text_similarity_new(\n",
    "    test_pairs, idf_dico_test, dict_tuple_mids_out)\n",
    "test_pairs['incoming_txt'] = textual_features.text_similarity_new(\n",
    "    test_pairs, idf_dico_test, dict_tuple_mids_in)\n",
    "\n",
    "# Greetings\n",
    "greeting_feature = np.empty(test_pairs['incoming_txt'].shape[0])\n",
    "index = 0\n",
    "for row in test_pairs.itertuples():\n",
    "    greeting_feature[index] = greeting_value(\n",
    "            row.body, row.recipient, greets, name)\n",
    "    index += 1\n",
    "test_pairs[\"greet\"] = greeting_feature\n",
    "\n",
    "test_pairs = test_pairs.rename(columns={\"sender\": \"user\", \"recipient\": \"contact\"})\n",
    "test_pairs = test_pairs[[\"user\", \"contact\", \"mid\", \"incoming_txt\", \"outgoing_txt\", \"greet\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "We train our dataset by looping on the number of senders in the test set.\n",
    "We fit one Random Forest for each sender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Training\"\n",
    "# Train arrays\n",
    "scores = []\n",
    "list_sender = np.unique(test_df['sender'].tolist())\n",
    "res_all = pd.DataFrame(columns=[\"mid\", \"contact\", \"recipients\"])\n",
    "for user in list_sender[107:]:\n",
    "    pairs_train_user = pairs_train[pairs_train.user == user]\n",
    "    X_train = pairs_train_user.merge(time_features, how=\"left\", on=[\"contact\", \"user\"]).merge(lda_df, how=\"left\", on=\"mid\")\n",
    "\n",
    "    X_train = X_train.fillna(0)\n",
    "    y_train = X_train[\"label\"].values\n",
    "    X_train = X_train.set_index([\"contact\", \"mid\", \"user\"])\n",
    "    X_train = X_train.drop([\"label\"], axis=1)\n",
    "    X_train = X_train.values\n",
    "\n",
    "    # Training\n",
    "    clf = RandomForestClassifier(n_estimators=50, random_state=42, oob_score=True, n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print clf.oob_score_\n",
    "\n",
    "    pairs_test_user = test_pairs[test_pairs.user == user]\n",
    "    # Getting the arrays for the prediction\n",
    "    X_test = pairs_test_user.merge(time_features, how=\"left\", on=[\"contact\", \"user\"]).merge(lda_df, how=\"left\", on=\"mid\")\n",
    "    X_test = X_test.fillna(0)\n",
    "    X_test = X_test.set_index([\"contact\", \"mid\", \"user\"])\n",
    "    test_index = X_test.index\n",
    "    X_test = X_test.values\n",
    "\n",
    "    print \"Predictions\"\n",
    "    # Predictions\n",
    "    pred = clf.predict_proba(X_test)[:, clf.classes_ == 1]\n",
    "    #pred = clf.predict(X_test)\n",
    "    pred = pd.DataFrame(pred, columns=[\"pred\"], index=test_index).reset_index()\n",
    "\n",
    "    # We take the top 10 for each mail\n",
    "    res = pred.groupby(\"mid\").apply(lambda row: row.sort_values(by=\"pred\", ascending=False).head(10)).reset_index(drop=True)\n",
    "    res = res[[\"mid\", \"contact\"]]\n",
    "    res = res.groupby(\"mid\").contact.apply(list).reset_index()\n",
    "    res[\"recipients\"] = res.contact.map(lambda x: ' '.join(x))\n",
    "    res_all = res_all.append(res)\n",
    "    res_all[\"mid\"] = res_all[\"mid\"].astype(np.int32)\n",
    "    # results\n",
    "    if TEST:\n",
    "        res = res.sort_values(by=\"mid\")\n",
    "        recips_test_user = recips_test[recips_test.mid.isin(res.mid)]\n",
    "        recips_test_user = recips_test_user.sort_values(by=\"mid\")\n",
    "        print mapk(recips_test_user[\"recipients\"].tolist(), res[\"contact\"].tolist())\n",
    "        scores.append(mapk(recips_test_user[\"recipients\"].tolist(), res[\"contact\"].tolist()))\n",
    "\n",
    "if TEST:\n",
    "    print \"Final mean score:\", np.mean(scores)\n",
    "    res_all.to_csv(\"results_time_text_all_DEV.csv\", columns=[\"mid\", \"recipients\"], index=False)\n",
    "else:\n",
    "    res_all.to_csv(\"results_time_text_all.csv\", columns=[\"mid\", \"recipients\"], index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
